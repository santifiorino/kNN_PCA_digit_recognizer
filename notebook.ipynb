{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea los k folds (Por cada uno crea 3 csv's: Train, Test y Expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(train, k):\n",
    "    train = train.sample(frac=1)\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append(train[i*len(train)//k : (i+1)*len(train)//k])\n",
    "    for i in range(k):\n",
    "        \n",
    "        expected = pd.DataFrame().assign(label=folds[i]['label'])\n",
    "        expected.to_csv('./k-fold/expected_' + str(i) + '.csv', index=False)\n",
    "\n",
    "        new_test = folds[i]\n",
    "        new_test.drop(['label'], axis=1).to_csv('./k-fold/test_' + str(i) + '.csv', index=False)\n",
    "\n",
    "        new_train = pd.concat(folds[:i] + folds[i+1:])\n",
    "        new_train.to_csv('./k-fold/train_' + str(i) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corre el algoritmo de PCA para cada uno de los folds, por cada fold crea un csv: Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA(folds, alpha, k):\n",
    "    for i in range(folds):\n",
    "        subprocess.run(['./PCA', f'./k-fold/train_{i}.csv', f'./k-fold/test_{i}.csv', f'./k-fold/pca/out_a{alpha}_k{k}_{i}.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kNN(folds, k, modo):\n",
    "    for i in range(folds):\n",
    "        subprocess.run(['./kNN', f'./k-fold/train_{i}.csv', f'./k-fold/test_{i}.csv', f'./k-fold/knn/out_{i}.csv', str(k), modo], stdout=subprocess.PIPE, encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "create_k_folds(train, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pecision, Recall, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(folds, output_file_path):\n",
    "    confusion = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(folds):\n",
    "        out = pd.read_csv(output_file_path + str(i) + '.csv')\n",
    "        expected = pd.read_csv(f'./k-fold/expected_{i}.csv')\n",
    "        for j in range(len(out)):\n",
    "            expected_label = int(expected.iloc[[j]]['label'])\n",
    "            predicted_label = int(out.iloc[[j]]['Label'])\n",
    "            confusion[expected_label][predicted_label] += 1\n",
    "    return confusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FP = np.sum(confusion, axis=0) - TP\n",
    "    FN = np.sum(confusion, axis=1) - TP\n",
    "    return TP / (TP+FP+FN)\n",
    "\n",
    "\n",
    "def get_precision(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FP = np.sum(confusion, axis=0) - TP\n",
    "    return TP / (TP+FP)\n",
    "\n",
    "\n",
    "def get_recall(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FN = np.sum(confusion, axis=1) - TP\n",
    "    return TP / (TP+FN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_for_pca = 3\n",
    "alphas = (1, 2, 3, 5, 8, 13, 21, 34, 55, 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alphas:\n",
    "    run_PCA(folds, alpha, k_for_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracies = []\n",
    "mean_precisions = []\n",
    "mean_recalls = []\n",
    "mean_f1_scores = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    confusion = confusion_matrix(\n",
    "        folds, f'./k-fold/pca/out_a{alpha}_k{k_for_pca}_')\n",
    "    accuracy = get_accuracy(confusion)\n",
    "    precision = get_precision(confusion)\n",
    "    recall = get_recall(confusion)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_f1_score = 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
    "    mean_accuracies.append(np.mean(accuracy))\n",
    "    mean_precisions.append(mean_precision)\n",
    "    mean_recalls.append(mean_recall)\n",
    "    mean_f1_scores.append(mean_f1_score)\n",
    "\n",
    "# Así se nos hace más fácil graficar\n",
    "accuracies = np.transpose(accuracies)\n",
    "precisions = np.transpose(precisions)\n",
    "recalls = np.transpose(recalls)\n",
    "f1_scores = np.transpose(f1_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, mean_accuracies, label='Accuracy')\n",
    "plt.plot(alphas, mean_precisions, label='Precision')\n",
    "plt.plot(alphas, mean_recalls, label='Recall')\n",
    "plt.plot(alphas, mean_f1_scores, label='F1-Score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"alpha\")\n",
    "plt.title(\"Efectividad media de PCA | k = 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(accuracies)):\n",
    "    accuracy_i = accuracies[i]\n",
    "    precision_i = precisions[i]\n",
    "    recall_i = recalls[i]\n",
    "    f1_score_i = f1_scores[i]\n",
    "    plt.figure()\n",
    "    plt.plot(alphas, accuracy_i, label='Accuracy')\n",
    "    plt.plot(alphas, precision_i, label='Precision')\n",
    "    plt.plot(alphas, recall_i, label='Recall')\n",
    "    plt.plot(alphas, f1_score_i, label='F1-Score')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.title(f\"Efectividad de PCA para dígito '{i}' | k = 3\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idem pero zoomeado\n",
    "for i in range(len(accuracies)):\n",
    "    accuracy_i = accuracies[i]\n",
    "    precision_i = precisions[i]\n",
    "    recall_i = recalls[i]\n",
    "    f1_score_i = f1_scores[i]\n",
    "    plt.figure()\n",
    "    plt.plot(alphas, accuracy_i, label='Accuracy')\n",
    "    plt.plot(alphas, precision_i, label='Precision')\n",
    "    plt.plot(alphas, recall_i, label='Recall')\n",
    "    plt.plot(alphas, f1_score_i, label='F1-Score')\n",
    "    plt.xlim(8, 90)\n",
    "    plt.ylim(0.6, 1)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"alpha\")\n",
    "    plt.title(f\"Efectividad de PCA para dígito '{i}' | k = 3\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos lo mismo pero variando el k en vez del alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 50\n",
    "ks = (1, 2, 3, 5, 8, 13, 21, 34, 55, 89)\n",
    "for k_for_pca in ks:\n",
    "    run_PCA(folds, alpha, k_for_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracies = []\n",
    "mean_precisions = []\n",
    "mean_recalls = []\n",
    "mean_f1_scores = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "for k in ks:\n",
    "    confusion = confusion_matrix(\n",
    "        folds, f'./k-fold/pca/out_a{alpha}_k{k}_')\n",
    "    accuracy = get_accuracy(confusion)\n",
    "    precision = get_precision(confusion)\n",
    "    recall = get_recall(confusion)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "    mean_precision = np.mean(precision)\n",
    "    mean_recall = np.mean(recall)\n",
    "    mean_f1_score = 2 * (mean_precision * mean_recall) / (mean_precision + mean_recall)\n",
    "    mean_accuracies.append(np.mean(accuracy))\n",
    "    mean_precisions.append(mean_precision)\n",
    "    mean_recalls.append(mean_recall)\n",
    "    mean_f1_scores.append(mean_f1_score)\n",
    "\n",
    "# Así se nos hace más fácil graficar\n",
    "accuracies = np.transpose(accuracies)\n",
    "precisions = np.transpose(precisions)\n",
    "recalls = np.transpose(recalls)\n",
    "f1_scores = np.transpose(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(alphas, mean_accuracies, label='Accuracy')\n",
    "plt.plot(alphas, mean_precisions, label='Precision')\n",
    "plt.plot(alphas, mean_recalls, label='Recall')\n",
    "plt.plot(alphas, mean_f1_scores, label='F1-Score')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.title(\"Efectividad media de PCA | alpha = 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(accuracies)):\n",
    "    accuracy_i = accuracies[i]\n",
    "    precision_i = precisions[i]\n",
    "    recall_i = recalls[i]\n",
    "    f1_score_i = f1_scores[i]\n",
    "    plt.figure()\n",
    "    plt.plot(alphas, accuracy_i, label='Accuracy')\n",
    "    plt.plot(alphas, precision_i, label='Precision')\n",
    "    plt.plot(alphas, recall_i, label='Recall')\n",
    "    plt.plot(alphas, f1_score_i, label='F1-Score')\n",
    "    plt.ylim(0.8, 1.0)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.xlabel(\"k\")\n",
    "    plt.title(f\"Efectividad de PCA para dígito '{i}' | alpha = 50\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pca = confusion_matrix(folds, './k-fold/pca/out_a50_k3_')\n",
    "accuracy = get_accuracy(confusion_pca)\n",
    "precision = get_precision(confusion_pca)\n",
    "recall = get_recall(confusion_pca)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_knn = confusion_matrix(folds, './k-fold/knn/out_')\n",
    "accuracy = get_accuracy(confusion_knn)\n",
    "precision = get_precision(confusion_knn)\n",
    "recall = get_recall(confusion_knn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confusion_pca, cmap='gray')\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.yticks(np.arange(0,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confusion_knn, cmap='gray')\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.yticks(np.arange(0,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa de Cohen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_labels(path):\n",
    "    labels = []\n",
    "    for i in range(folds):\n",
    "        out = pd.read_csv(path + str(i) + '.csv')\n",
    "        labels += out['Label'].tolist()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pred = output_labels('./k-fold/pca/out_a50_k3_')\n",
    "knn_pred = output_labels('./k-fold/knn/out_')\n",
    "cohen_kappa_score(pca_pred, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(sub, correct_answers):\n",
    "    count = 0\n",
    "    for i in range(len(sub)):\n",
    "        if int(sub.iloc[[i]]['Label']) == int(correct_answers.iloc[[i]]['Label']):\n",
    "            count += 1\n",
    "    return count / len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('kaggle.csv')\n",
    "\n",
    "alphas = list(range(1, 21)) + [25, 30, 35, 40, 45, 50]\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 20, 25, 30, 40, 50, 100]\n",
    "\n",
    "data = pd.DataFrame([], columns=['alpha', 'k', 'accuracy', 'time'])\n",
    "for alpha in alphas:\n",
    "    for k in ks:\n",
    "        time = subprocess.run(['./PCA', 'train.csv', 'test.csv', 'out.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "        score = get_accuracy(pd.read_csv('out.csv'), kaggle)\n",
    "        data = data.append({'alpha': alpha, 'k': k, 'accuracy': score, 'time': float(time)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/results/PCA_accuracy.csv').astype({'alpha': 'int32', 'k': 'int32'})\n",
    "sns.relplot(data=data, x=\"alpha\", y=\"accuracy\", hue=\"k\", aspect=1.5, legend=\"full\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"α y k vs. Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = pd.read_csv('neighbors.csv')\n",
    "neighbors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = np.zeros((len(test), 10))\n",
    "neighbor_count = 0\n",
    "accuracies = []\n",
    "for k in ks:\n",
    "    accuracy = 0\n",
    "    for i in range(len(test)):\n",
    "        curr_neighbors = neighbors.loc[neighbors[\"ImageId\"] == i+1][neighbor_count:k]\n",
    "        for j in range(len(curr_neighbors)):\n",
    "            curr_neighbor = curr_neighbors.iloc[[j]]\n",
    "            distance_matrix[i][curr_neighbor[\"Class\"]] += 1 / curr_neighbor[\"Distance\"]\n",
    "        if distance_matrix[i].argmax() == int(kaggle.iloc[[i]]['Label']):\n",
    "            accuracy += 1\n",
    "    neighbor_count = k\n",
    "    accuracies.append(accuracy / len(test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data[data[\"k\"].isin(range(20))]\n",
    "data3 = data3[data3[\"alpha\"].isin(range(16, 50))]\n",
    "sns.relplot(data=data3, x=\"k\", y=\"accuracy\", hue=\"alpha\", kind=\"line\", aspect=1.5, legend=\"full\")\n",
    "plt.plot(ks[:8], accuracies[:8], marker=\"o\", label=\"kNN\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy de kNN vs PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy vs Training Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [100, 500, 1000, 2500, 5000, 10000, 25000, 40000]\n",
    "for size in training_sizes:\n",
    "    train_subset = pd.DataFrame()\n",
    "    new_train_index = []\n",
    "    for i in range(10):\n",
    "        new_train_index += train.index[train['label'] == i][:size//10].tolist()\n",
    "    new_train = train.loc[new_train_index]\n",
    "    new_train.to_csv(f'./train_subsets/train_subset_{size}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_accuracy = []\n",
    "knn_accuracy = []\n",
    "for size in training_sizes:\n",
    "    subprocess.run(['./PCA', f'./train_subsets/train_subset_{size}.csv', 'test.csv', 'out2.csv', \"50\", \"3\"], stdout=subprocess.PIPE, encoding='ascii')\n",
    "    sub = pd.read_csv('./out2.csv')\n",
    "    pca_accuracy.append(get_accuracy(sub, kaggle))\n",
    "    subprocess.run(['./knn', f'./train_subsets/train_subset_{size}.csv', 'test.csv', 'out2.csv', \"3\", '1'], stdout=subprocess.PIPE, encoding='ascii')\n",
    "    sub = pd.read_csv('./out2.csv')\n",
    "    knn_accuracy.append(get_accuracy(sub, kaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(training_sizes[2:], pca_accuracy[2:], marker='.')\n",
    "plt.plot(training_sizes[2:], knn_accuracy[2:], marker='.')\n",
    "plt.xlabel('Imágenes de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([1000, 5000, 10000, 25000, 40000])\n",
    "plt.legend(['PCA', 'kNN'])\n",
    "plt.title('Cantidad de Imágenes de entrenamiento vs. Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "knn_time = []\n",
    "for k in ks:\n",
    "    print(k)\n",
    "    curr_knn = []\n",
    "    for i in range(10):\n",
    "        time = subprocess.run(['./kNN', './train_subsets/train_subset_5000.csv', 'test_subset_3000.csv', 'out2.csv', str(k), '0'], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "        curr_knn.append(time)\n",
    "    knn_time.append(curr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_time = np.array(knn_time).astype(float)\n",
    "average_times = np.mean(knn_time, axis=1)\n",
    "plt.plot(ks, average_times, marker='.')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"k vs. Tiempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1, 10, 25, 50, 100]\n",
    "ks = [1, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "pca_times = pd.DataFrame(columns=['alpha', 'k', 'time'])\n",
    "for k in ks:\n",
    "    for alpha_index, alpha in enumerate(alphas):\n",
    "        for i in range(10):\n",
    "            time = subprocess.run(['./PCA', './train_subsets/train_subset_5000.csv', 'test_subset_3000.csv', 'out2.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "            pca_times.loc[len(pca_times)] = [alpha, k, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_time_data = pd.read_csv(\"./results/PCA_time.csv\")\n",
    "pca_graph_data = pd.DataFrame(columns=['alpha', 'k', 'time'])\n",
    "for k in ks:\n",
    "    for alpha in alphas:\n",
    "        curr_data = pca_time_data[(pca_time_data['alpha'] == alpha) & (pca_time_data['k'] == k)]\n",
    "        avg_distance = curr_data['time'].mean()\n",
    "        pca_graph_data.loc[len(pca_graph_data)] = [alpha, k, avg_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, pca_graph_data[pca_graph_data[\"alpha\"] == 50][\"time\"], marker=\"o\", color=\"orange\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"k vs. Tiempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=pca_graph_data.astype({'alpha': 'int32', 'k': 'int32'}), x=\"alpha\", y=\"time\", hue=\"k\", aspect=1.25, kind=\"line\", legend=\"full\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"α y  k vs. Tiempo\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30295c5bec572e859485b1ffa5e89b8b3e2022ef6e3e739c1ac40f143a557caf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
