{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "import seaborn as sns\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crea los k folds (Por cada uno crea 3 csv's: Train, Test y Expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_k_folds(train, k):\n",
    "    train = train.sample(frac=1)\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        folds.append(train[i*len(train)//k : (i+1)*len(train)//k])\n",
    "    for i in range(k):\n",
    "        \n",
    "        expected = pd.DataFrame().assign(label=folds[i]['label'])\n",
    "        expected.to_csv('./k-fold/expected_' + str(i) + '.csv', index=False)\n",
    "\n",
    "        new_test = folds[i]\n",
    "        new_test.drop(['label'], axis=1).to_csv('./k-fold/test_' + str(i) + '.csv', index=False)\n",
    "\n",
    "        new_train = pd.concat(folds[:i] + folds[i+1:])\n",
    "        new_train.to_csv('./k-fold/train_' + str(i) + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corre el algoritmo de PCA para cada uno de los folds, por cada fold crea un csv: Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PCA(folds, alpha, k):\n",
    "    for i in range(folds):\n",
    "        subprocess.run(['./PCA', f'./k-fold/train_{i}.csv', f'./k-fold/test_{i}.csv', f'./k-fold/pca/out_{i}.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kNN(folds, k):\n",
    "    for i in range(folds):\n",
    "        subprocess.run(['./kNN', f'./k-fold/train_{i}.csv', f'./k-fold/test_{i}.csv', f'./k-fold/knn/out_{i}.csv', str(k)], stdout=subprocess.PIPE, encoding='ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = 10\n",
    "create_k_folds(train, folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pecision, Recall, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 4\n",
    "k_for_pca = 5\n",
    "run_PCA(folds, alpha, k_for_pca)\n",
<<<<<<< HEAD
    "k_for_knn = 1\n",
=======
    "k_for_knn = -1 # TODO falta buscar cual es el k optimo para knn\n",
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
    "run_kNN(folds, k_for_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(folds, method):\n",
    "    confusion = np.zeros((10, 10), dtype=int)\n",
    "    for i in range(folds):\n",
    "        out = pd.read_csv(f'./k-fold/{method}/out_{i}.csv')\n",
    "        expected = pd.read_csv(f'./k-fold/expected_{i}.csv')\n",
    "        for j in range(len(out)):\n",
    "            expected_label = int(expected.iloc[[j]]['label'])\n",
    "            predicted_label = int(out.iloc[[j]]['Label'])\n",
    "            confusion[expected_label][predicted_label] += 1\n",
    "    return confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FP = np.sum(confusion, axis=0) - TP\n",
    "    FN = np.sum(confusion, axis=1) - TP\n",
    "    return TP/(TP+FP+FN)\n",
    "\n",
    "def get_precision(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FP = np.sum(confusion, axis=0) - TP\n",
    "    return TP / (TP+FP)\n",
    "\n",
    "def get_recall(confusion):\n",
    "    TP = np.diag(confusion)\n",
    "    FN = np.sum(confusion, axis=1) - TP\n",
<<<<<<< HEAD
    "    return TP / (TP+FN)"
=======
    "    return TP / (TP+FN)\n"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_pca = confusion_matrix(folds, 'pca')\n",
    "accuracy = get_accuracy(confusion_pca)\n",
    "precision = get_precision(confusion_pca)\n",
    "recall = get_recall(confusion_pca)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_knn = confusion_matrix(folds, 'knn')\n",
    "accuracy = get_accuracy(confusion_knn)\n",
    "precision = get_precision(confusion_knn)\n",
    "recall = get_recall(confusion_knn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1_score)"
<<<<<<< HEAD
=======
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(confusion_pca, cmap='gray')\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.yticks(np.arange(0,10));"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "plt.imshow(confusion_pca, cmap='gray')\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.yticks(np.arange(0,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
    "plt.imshow(confusion_knn, cmap='gray')\n",
    "plt.xticks(np.arange(0,10))\n",
    "plt.yticks(np.arange(0,10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Kappa de Cohen"
=======
    "### Kappa de Cohen\n",
    "The function cohen_kappa_score computes Cohenâ€™s kappa statistic. This measure is intended to compare labelings by different classifiers, not a classifier versus a ground truth.\n",
    "\n",
    "The kappa score is a number between -1 and 1. Scores above .8 are generally considered good agreement; zero or lower means no agreement (practically random labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_labels(method):\n",
    "    labels = []\n",
    "    for i in range(folds):\n",
    "        out = pd.read_csv(f'./k-fold/{method}/out_{i}.csv')\n",
    "        labels += out['Label'].tolist()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pred = output_labels('pca')\n",
    "knn_pred = output_labels('knn')\n",
    "cohen_kappa_score(pca_pred, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA metrics"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_labels(method):\n",
    "    labels = []\n",
    "    for i in range(folds):\n",
    "        out = pd.read_csv(f'./k-fold/{method}/out_{i}.csv')\n",
    "        labels += out['Label'].tolist()\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pred = output_labels('pca')\n",
    "knn_pred = output_labels('knn')\n",
    "cohen_kappa_score(pca_pred, knn_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(sub, correct_answers):\n",
    "    count = 0\n",
    "    for i in range(len(sub)):\n",
    "        if int(sub.iloc[[i]]['Label']) == int(correct_answers.iloc[[i]]['Label']):\n",
    "            count += 1\n",
    "    return count / len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('kaggle.csv')\n",
    "\n",
    "alphas = list(range(1, 21)) + [25, 30, 35, 40, 45, 50]\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 20, 25, 30, 40, 50, 100]\n",
    "\n",
    "data = pd.DataFrame([], columns=['alpha', 'k', 'accuracy', 'time'])\n",
    "for alpha in alphas:\n",
    "    for k in ks:\n",
    "        time = subprocess.run(['./PCA', 'train.csv', 'test.csv', 'out.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "        score = get_accuracy(pd.read_csv('out.csv'), kaggle)\n",
    "        data = data.append({'alpha': alpha, 'k': k, 'accuracy': score, 'time': float(time)}, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "data = pd.read_csv('/results/PCA_accuracy.csv').astype({'alpha': 'int32', 'k': 'int32'})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=data, x=\"alpha\", y=\"accuracy\", hue=\"k\", aspect=1.5, legend=\"full\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Î± y  k vs. Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
=======
    "data.head()"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "neighbors = pd.read_csv('neighbors.csv')\n",
    "neighbors.head()"
=======
    "sns.relplot(data=data, x=\"alpha\", y=\"accuracy\", hue=\"k\", aspect=1.5)"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "distance_matrix = np.zeros((len(test), 10))\n",
    "neighbor_count = 0\n",
    "accuracies = []\n",
    "for k in ks:\n",
    "    accuracy = 0\n",
    "    for i in range(len(test)):\n",
    "        curr_neighbors = neighbors.loc[neighbors[\"ImageId\"] == i+1][neighbor_count:k]\n",
    "        for j in range(len(curr_neighbors)):\n",
    "            curr_neighbor = curr_neighbors.iloc[[j]]\n",
    "            distance_matrix[i][curr_neighbor[\"Class\"]] += 1 / curr_neighbor[\"Distance\"]\n",
    "        if distance_matrix[i].argmax() == int(kaggle.iloc[[i]]['Label']):\n",
    "            accuracy += 1\n",
    "    neighbor_count = k\n",
    "    accuracies.append(accuracy / len(test)) "
=======
    "p = sns.relplot(data=data, x=\"alpha\", y=\"time\", hue=\"k\", kind=\"line\", aspect=1.5)\n",
    "p.set(xlabel='alpha', ylabel='time [seg]')"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "data3 = data[data[\"k\"].isin(range(20))]\n",
    "data3 = data3[data3[\"alpha\"].isin(range(16, 50))]\n",
    "sns.relplot(data=data3, x=\"k\", y=\"accuracy\", hue=\"alpha\", kind=\"line\", aspect=1.5, legend=\"full\")\n",
    "plt.plot(ks[:8], accuracies[:8], marker=\"o\", label=\"kNN\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy de kNN vs PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy vs Training Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [100, 500, 1000, 2500, 5000, 10000, 25000, 40000]\n",
    "for size in training_sizes:\n",
    "    train_subset = pd.DataFrame()\n",
    "    new_train_index = []\n",
    "    for i in range(10):\n",
    "        new_train_index += train.index[train['label'] == i][:size//10].tolist()\n",
    "    new_train = train.loc[new_train_index]\n",
    "    new_train.to_csv(f'./train_subsets/train_subset_{size}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_accuracy = []\n",
    "knn_accuracy = []\n",
    "for size in training_sizes:\n",
    "    subprocess.run(['./PCA', f'./train_subsets/train_subset_{size}.csv', 'test.csv', 'out2.csv', \"50\", \"3\"], stdout=subprocess.PIPE, encoding='ascii')\n",
    "    sub = pd.read_csv('./out2.csv')\n",
    "    pca_accuracy.append(get_accuracy(sub, kaggle))\n",
    "    subprocess.run(['./knn', f'./train_subsets/train_subset_{size}.csv', 'test.csv', 'out2.csv', \"3\"], stdout=subprocess.PIPE, encoding='ascii')\n",
    "    sub = pd.read_csv('./out2.csv')\n",
    "    knn_accuracy.append(get_accuracy(sub, kaggle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(training_sizes[2:], pca_accuracy[2:], marker='.')\n",
    "plt.plot(training_sizes[2:], knn_accuracy[2:], marker='.')\n",
    "plt.xlabel('ImÃ¡genes de entrenamiento')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks([1000, 5000, 10000, 25000, 40000])\n",
    "plt.legend(['PCA', 'kNN'])\n",
    "plt.title('Cantidad de ImÃ¡genes de entrenamiento vs. Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [1, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "knn_time = []\n",
    "for k in ks:\n",
    "    print(k)\n",
    "    curr_knn = []\n",
    "    for i in range(10):\n",
    "        time = subprocess.run(['./kNN_tiempos', './train_subsets/train_subset_5000.csv', 'test_subset_3000.csv', 'out2.csv', str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "        curr_knn.append(time)\n",
    "    knn_time.append(curr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_time = np.array(knn_time).astype(float)\n",
    "average_times = np.mean(knn_time, axis=1)\n",
    "plt.plot(ks, average_times, marker='.')\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"k vs. Tiempo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [1, 10, 25, 50, 100]\n",
    "ks = [1, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "pca_times = pd.DataFrame(columns=['alpha', 'k', 'time'])\n",
    "for k in ks:\n",
    "    for alpha_index, alpha in enumerate(alphas):\n",
    "        for i in range(10):\n",
    "            time = subprocess.run(['./PCA', './train_subsets/train_subset_5000.csv', 'test_subset_3000.csv', 'out2.csv', str(alpha), str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "            pca_times.loc[len(pca_times)] = [alpha, k, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_time_data = pd.read_csv(\"./results/PCA_time.csv\")\n",
    "pca_graph_data = pd.DataFrame(columns=['alpha', 'k', 'time'])\n",
    "for k in ks:\n",
    "    for alpha in alphas:\n",
    "        curr_data = pca_time_data[(pca_time_data['alpha'] == alpha) & (pca_time_data['k'] == k)]\n",
    "        avg_distance = curr_data['time'].mean()\n",
    "        pca_graph_data.loc[len(pca_graph_data)] = [alpha, k, avg_distance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ks, pca_graph_data[pca_graph_data[\"alpha\"] == 50][\"time\"], marker=\"o\", color=\"orange\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"k vs. Tiempo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=pca_graph_data.astype({'alpha': 'int32', 'k': 'int32'}), x=\"alpha\", y=\"time\", hue=\"k\", aspect=1.25, kind=\"line\", legend=\"full\")\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Tiempo [s]\")\n",
    "plt.title(\"Î± y  k vs. Tiempo\")"
=======
    "# El mejor es:\n",
    "column = data[\"accuracy\"]\n",
    "max_index = column.idxmax()\n",
    "data.iloc[[max_index]]"
>>>>>>> d448a81ec7f6f02f717c71d1006c39ee7449bf0a
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle = pd.read_csv('kaggle.csv')\n",
    "\n",
    "ks = [1, 3, 5, 7, 9, 11, 13, 15, 20, 25, 30, 40, 50, 100]\n",
    "\n",
    "data = pd.DataFrame([], columns=['k', 'accuracy', 'time'])\n",
    "i = 0\n",
    "start = time.time()\n",
    "time_previous = 0\n",
    "for k in ks:\n",
    "    clear_output(wait=True)\n",
    "    display(f'kNN tiempos - Experimento: {i+1} / {len(ks)} - Tiempo (dataset): {time.time() - start} segs - Tiempo (Ãºltimo): {time_previous}')        \n",
    "    time_experiment = subprocess.run(['./kNN_tiempos', 'train.csv', 'test.csv', 'out.csv', str(k)], stdout=subprocess.PIPE, encoding='ascii').stdout.split('\\n')[0]\n",
    "    score = get_kaggle_accuracy(pd.read_csv('out.csv'), kaggle)\n",
    "    data = data.append({'k': k, 'accuracy': score, 'time': float(time)}, ignore_index = True)\n",
    "    time_previous = time_experiment\n",
    "    i += 1\n",
    "data.to_csv('knn_accuracy_time.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
